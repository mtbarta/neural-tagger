{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from NLPutils.models.dilatedconv import DilatedConv\n",
    "from NLPutils.models.dconv2 import DConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/src/NLPutils/trainers/dconv2.py:55: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(length == len(guess[b]), \"lengths differ: length-- {}, len(guess[b])-- {} \".format(length, len(guess[b])))\n"
     ]
    }
   ],
   "source": [
    "# from NLPutils.trainers.dconv_trainer import DConvTrainer\n",
    "from NLPutils.trainers.dconv2 import DConvTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = '/data/ner/conll/eng.train'\n",
    "valid = '/data/ner/conll/eng.testa'\n",
    "test = '/data/ner/conll/eng.testb'\n",
    "word_embed_loc = '/data/embeddings/GoogleNews-vectors-negative300.bin'\n",
    "valsplit=0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14986\n",
      "Loaded  training data\n",
      "Using provided validation data\n",
      "Loaded test data\n"
     ]
    }
   ],
   "source": [
    "from NLPutils.util.conll_util import *\n",
    "from NLPutils.embeddings import Word2VecModel, RandomInitVecModel\n",
    "\n",
    "maxs, maxw, vocab_ch, vocab_word = conllBuildVocab([train, valid,\n",
    "                                                    test])\n",
    "\n",
    "# Vocab LUTs\n",
    "word_vocab = None\n",
    "char_vocab = None\n",
    "\n",
    "word_vec = Word2VecModel(word_embed_loc, vocab_word, 0.25)\n",
    "word_vocab = word_vec.vocab\n",
    "\n",
    "# if FLAGS.charsz != FLAGS.wsz and FLAGS.cbow is True:\n",
    "#     print('Warning, you have opted for CBOW char embeddings, but have provided differing sizes for char embedding depth and word depth.  This is not possible, forcing char embedding depth to be word depth ' + FLAGS.wsz)\n",
    "#     FLAGS.charsz = FLAGS.wsz\n",
    "\n",
    "char_vec = RandomInitVecModel(16, vocab_ch, 0.25)\n",
    "char_vocab = char_vec.vocab\n",
    "\n",
    "f2i = {\"<PAD>\":0}\n",
    "\n",
    "ts, f2i, _ = conllSentsToIndices(train, word_vocab, char_vocab, maxs, maxw, f2i, 3)\n",
    "print(len(ts))\n",
    "print('Loaded  training data')\n",
    "\n",
    "if valid is not None:\n",
    "    print('Using provided validation data')\n",
    "    vs, f2i,_ = conllSentsToIndices(valid, word_vocab, char_vocab, maxs, maxw, f2i, 3)\n",
    "else:\n",
    "    ts, vs = validSplit(ts, valsplit)\n",
    "    print('Created validation split')\n",
    "\n",
    "\n",
    "es, f2i,txts = conllSentsToIndices(test, word_vocab, char_vocab, maxs, maxw, f2i, 3)\n",
    "print('Loaded test data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embed weights shape:  (30291, 300)\n",
      "max sentence len 124\n"
     ]
    }
   ],
   "source": [
    "print('word embed weights shape: ', word_vec.weights.shape)\n",
    "print( 'max sentence len', maxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crf=True, creating SLL\n",
      "crf=True, creating SLL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up word embedding visualization\n",
      "Writing metadata\n",
      "Training epoch 1.\n",
      "Train (Loss 0.1324) (169.546 sec)\n",
      "Validation (F1 = 0.7211) (Acc 49069/51409 = 0.9545) (8.956 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 2.\n",
      "\t(last improvement @ 1)\n",
      "Train (Loss 0.0423) (161.183 sec)\n",
      "Validation (F1 = 0.8777) (Acc 50338/51409 = 0.9792) (8.697 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 3.\n",
      "\t(last improvement @ 2)\n",
      "Train (Loss 0.0238) (160.942 sec)\n",
      "Validation (F1 = 0.8911) (Acc 50471/51409 = 0.9818) (8.799 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 4.\n",
      "\t(last improvement @ 3)\n",
      "Train (Loss 0.0159) (161.319 sec)\n",
      "Validation (F1 = 0.9083) (Acc 50601/51409 = 0.9843) (8.820 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 5.\n",
      "\t(last improvement @ 4)\n",
      "Train (Loss 0.0116) (160.972 sec)\n",
      "Validation (F1 = 0.9123) (Acc 50657/51409 = 0.9854) (8.738 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 6.\n",
      "\t(last improvement @ 5)\n",
      "Train (Loss 0.0083) (161.436 sec)\n",
      "Validation (F1 = 0.9153) (Acc 50678/51409 = 0.9858) (8.677 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 7.\n",
      "\t(last improvement @ 6)\n",
      "Train (Loss 0.0066) (160.027 sec)\n",
      "Validation (F1 = 0.9188) (Acc 50721/51409 = 0.9866) (8.793 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 8.\n",
      "\t(last improvement @ 7)\n",
      "Train (Loss 0.0054) (160.306 sec)\n",
      "Validation (F1 = 0.9212) (Acc 50721/51409 = 0.9866) (8.670 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 9.\n",
      "\t(last improvement @ 8)\n",
      "Train (Loss 0.0046) (161.089 sec)\n",
      "Validation (F1 = 0.9188) (Acc 50709/51409 = 0.9864) (8.759 sec)\n",
      "Training epoch 10.\n",
      "\t(last improvement @ 8)\n",
      "Train (Loss 0.0038) (159.428 sec)\n",
      "Validation (F1 = 0.9166) (Acc 50673/51409 = 0.9857) (8.818 sec)\n",
      "Training epoch 11.\n",
      "\t(last improvement @ 8)\n",
      "Train (Loss 0.0038) (159.103 sec)\n",
      "Validation (F1 = 0.9168) (Acc 50709/51409 = 0.9864) (8.681 sec)\n",
      "Training epoch 12.\n",
      "\t(last improvement @ 8)\n",
      "Train (Loss 0.0030) (158.796 sec)\n",
      "Validation (F1 = 0.9225) (Acc 50731/51409 = 0.9868) (8.813 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 13.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0032) (158.352 sec)\n",
      "Validation (F1 = 0.9180) (Acc 50681/51409 = 0.9858) (8.669 sec)\n",
      "Training epoch 14.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0029) (157.090 sec)\n",
      "Validation (F1 = 0.9097) (Acc 50600/51409 = 0.9843) (8.662 sec)\n",
      "Training epoch 15.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0026) (157.286 sec)\n",
      "Validation (F1 = 0.9101) (Acc 50625/51409 = 0.9847) (8.653 sec)\n",
      "Training epoch 16.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0025) (156.714 sec)\n",
      "Validation (F1 = 0.9114) (Acc 50601/51409 = 0.9843) (8.649 sec)\n",
      "Training epoch 17.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0025) (156.516 sec)\n",
      "Validation (F1 = 0.9147) (Acc 50665/51409 = 0.9855) (8.647 sec)\n",
      "Training epoch 18.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0030) (156.633 sec)\n",
      "Validation (F1 = 0.9095) (Acc 50639/51409 = 0.9850) (8.646 sec)\n",
      "Training epoch 19.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0027) (156.498 sec)\n",
      "Validation (F1 = 0.9131) (Acc 50630/51409 = 0.9848) (8.643 sec)\n",
      "Training epoch 20.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0023) (156.490 sec)\n",
      "Validation (F1 = 0.9065) (Acc 50506/51409 = 0.9824) (8.653 sec)\n",
      "Training epoch 21.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0024) (156.473 sec)\n",
      "Validation (F1 = 0.9146) (Acc 50630/51409 = 0.9848) (8.642 sec)\n",
      "Training epoch 22.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0021) (156.439 sec)\n",
      "Validation (F1 = 0.8863) (Acc 50398/51409 = 0.9803) (8.657 sec)\n",
      "Training epoch 23.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0024) (156.368 sec)\n",
      "Validation (F1 = 0.9062) (Acc 50560/51409 = 0.9835) (8.650 sec)\n",
      "Training epoch 24.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0975) (156.336 sec)\n",
      "Validation (F1 = 0.9128) (Acc 50611/51409 = 0.9845) (8.645 sec)\n",
      "Training epoch 25.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0021) (156.464 sec)\n",
      "Validation (F1 = 0.9090) (Acc 50601/51409 = 0.9843) (8.666 sec)\n",
      "Training epoch 26.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0029) (156.332 sec)\n",
      "Validation (F1 = 0.8966) (Acc 50481/51409 = 0.9819) (8.643 sec)\n",
      "Training epoch 27.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0023) (156.319 sec)\n",
      "Validation (F1 = 0.9099) (Acc 50646/51409 = 0.9852) (8.645 sec)\n",
      "Training epoch 28.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0042) (156.247 sec)\n",
      "Validation (F1 = 0.9117) (Acc 50569/51409 = 0.9837) (8.645 sec)\n",
      "Training epoch 29.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0040) (159.805 sec)\n",
      "Validation (F1 = 0.9076) (Acc 50587/51409 = 0.9840) (9.061 sec)\n",
      "Training epoch 30.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0027) (159.664 sec)\n",
      "Validation (F1 = 0.8985) (Acc 50508/51409 = 0.9825) (8.644 sec)\n",
      "Training epoch 31.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0027) (157.077 sec)\n",
      "Validation (F1 = 0.9151) (Acc 50632/51409 = 0.9849) (8.659 sec)\n",
      "Training epoch 32.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0033) (157.447 sec)\n",
      "Validation (F1 = 0.9057) (Acc 50524/51409 = 0.9828) (8.642 sec)\n",
      "Training epoch 33.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0026) (156.750 sec)\n",
      "Validation (F1 = 0.9022) (Acc 50509/51409 = 0.9825) (8.650 sec)\n",
      "Training epoch 34.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0019) (157.514 sec)\n",
      "Validation (F1 = 0.9102) (Acc 50573/51409 = 0.9837) (8.707 sec)\n",
      "Training epoch 35.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0024) (157.192 sec)\n",
      "Validation (F1 = 0.9045) (Acc 50562/51409 = 0.9835) (8.762 sec)\n",
      "Training epoch 36.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0021) (157.844 sec)\n",
      "Validation (F1 = 0.9046) (Acc 50532/51409 = 0.9829) (8.709 sec)\n",
      "Training epoch 37.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0025) (156.815 sec)\n",
      "Validation (F1 = 0.8858) (Acc 50269/51409 = 0.9778) (8.756 sec)\n",
      "Training epoch 38.\n",
      "\t(last improvement @ 12)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-77c46557291b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m              \u001b[0mfscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m               \u001b[0mviz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m              crf=True)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#1 -- 86.55 dropout at .45 and 300 filters. stopped at epoc 24.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/NLPutils/trainers/dconv2.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, name, ts, f2i, vs, es, char_vec, word_vec, eval_out, batchsz, epochs, dropout, test_thresh, patience, rnn, maxlen, maxw, wsz, hsz, cfiltsz, optim, eta, crf, fscore, viz, clip, kernel_size, num_layers, num_iterations, word_keep, num_filt)\u001b[0m\n\u001b[1;32m    276\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t(last improvement @ %d)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlast_improved\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_keep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m                             \u001b[0mthis_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/NLPutils/trainers/dconv2.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, ts, dropout, batchsz, model, sess, word_keep)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mex2dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_keep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "trainer = DConvTrainer(None, DConv, 'conll-ner-dconv-7')\n",
    "trainer.train('ner', ts, f2i, vs, es,\n",
    "             char_vec,\n",
    "             word_vec,\n",
    "             'eval',\n",
    "              batchsz=2**5,\n",
    "              optim='adam',\n",
    "              eta=0.0005,\n",
    "              epochs=500,\n",
    "              dropout=0.15,\n",
    "              patience=50,\n",
    "              cfiltsz='1,3,5,7',\n",
    "             maxlen=maxs,\n",
    "             maxw=maxw,\n",
    "              num_filt=300,\n",
    "             num_layers=3,\n",
    "             num_iterations=2,\n",
    "             fscore=1,\n",
    "              viz=1,\n",
    "             crf=True)\n",
    "\n",
    "#1 -- 86.55 dropout at .45 and 300 filters. stopped at epoc 24.\n",
    "#2 -- 86.7 dropout at .35, 350 filters.\n",
    "#3 -- added dilation 1 conv at end of block. 86.28\n",
    "#4 -- 3 layers, 3 iters. got much worse. peeked at 91.8 in training, started dropping.\n",
    "#5 -- layers in block should be 1,2,1 from strubell's code. lowered dropout to .15 -- layers would have been 1,1,1.\n",
    "#6 -- clipped gradients'\n",
    "#7 -- 3 layers, 2 iterations."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
