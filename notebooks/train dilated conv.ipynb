{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=9\n",
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8571471034767092337\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10978911847\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 4744660123513609239\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:0f:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=9\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print (device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from NLPutils.models.dilatedconv import DilatedConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from NLPutils.trainers.dconv_trainer import DConvTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = '/data/datasets/ner/eng.train'\n",
    "valid = '/data/datasets/ner/eng.testa'\n",
    "test = '/data/datasets/ner/eng.testb'\n",
    "word_embed_loc = '/data/embeddings/GoogleNews-vectors-negative300.bin'\n",
    "valsplit=0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14986\n",
      "Loaded  training data\n",
      "Using provided validation data\n",
      "Loaded test data\n"
     ]
    }
   ],
   "source": [
    "from src.util.conll_util import *\n",
    "from src.embeddings import Word2VecModel, RandomInitVecModel\n",
    "\n",
    "maxs, maxw, vocab_ch, vocab_word = conllBuildVocab([train, valid,\n",
    "                                                    test])\n",
    "\n",
    "# Vocab LUTs\n",
    "word_vocab = None\n",
    "char_vocab = None\n",
    "\n",
    "word_vec = Word2VecModel(word_embed_loc, vocab_word, 0.25)\n",
    "word_vocab = word_vec.vocab\n",
    "\n",
    "# if FLAGS.charsz != FLAGS.wsz and FLAGS.cbow is True:\n",
    "#     print('Warning, you have opted for CBOW char embeddings, but have provided differing sizes for char embedding depth and word depth.  This is not possible, forcing char embedding depth to be word depth ' + FLAGS.wsz)\n",
    "#     FLAGS.charsz = FLAGS.wsz\n",
    "\n",
    "char_vec = RandomInitVecModel(16, vocab_ch, 0.25)\n",
    "char_vocab = char_vec.vocab\n",
    "\n",
    "f2i = {\"<PAD>\":0}\n",
    "\n",
    "ts, f2i, _ = conllSentsToIndices(train, word_vocab, char_vocab, maxs, maxw, f2i, 3)\n",
    "print(len(ts))\n",
    "print('Loaded  training data')\n",
    "\n",
    "if valid is not None:\n",
    "    print('Using provided validation data')\n",
    "    vs, f2i,_ = conllSentsToIndices(valid, word_vocab, char_vocab, maxs, maxw, f2i, 3)\n",
    "else:\n",
    "    ts, vs = validSplit(ts, valsplit)\n",
    "    print('Created validation split')\n",
    "\n",
    "\n",
    "es, f2i,txts = conllSentsToIndices(test, word_vocab, char_vocab, maxs, maxw, f2i, 3)\n",
    "print('Loaded test data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embed weights shape:  (30291, 300)\n",
      "max sentence len 124\n"
     ]
    }
   ],
   "source": [
    "print('word embed weights shape: ', word_vec.weights.shape)\n",
    "print( 'max sentence len', maxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/src/src/trainers/dconv3.py:55: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(length == len(guess[b]), \"lengths differ: length-- {}, len(guess[b])-- {} \".format(length, len(guess[b])))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_feats Tensor(\"ner/ExpandDims:0\", shape=(?, 1, 124, 420), dtype=float32)\n",
      "filter_shape [1, 3, Dimension(420), 300]\n",
      "h0 Tensor(\"ner/relu:0\", shape=(?, 1, 124, 300), dtype=float32)\n",
      "last_output Tensor(\"ner/concat_1:0\", shape=(?, 1, 124, 300), dtype=float32)\n",
      "block output Tensor(\"ner/block/iterated-block/relu_2:0\", shape=(?, ?, ?, 300), dtype=float32)\n",
      "h_concat_squeeze Tensor(\"ner/block/Squeeze:0\", shape=(?, ?, 300), dtype=float32)\n",
      "h_concat_flat Tensor(\"ner/block/Reshape:0\", shape=(?, 300), dtype=float32)\n",
      "input_to_pred Tensor(\"ner/block/hidden_dropout/dropout/mul:0\", shape=(?, 300), dtype=float32)\n",
      "proj_width 300\n",
      "scores Tensor(\"ner/block/output/scores:0\", shape=(?, 9), dtype=float32)\n",
      "unflat_scores Tensor(\"ner/block/output/Reshape:0\", shape=(?, 124, 9), dtype=float32)\n",
      "block output Tensor(\"ner/block_1/iterated-block/relu_2:0\", shape=(?, ?, ?, 300), dtype=float32)\n",
      "h_concat_squeeze Tensor(\"ner/block_1/Squeeze:0\", shape=(?, ?, 300), dtype=float32)\n",
      "h_concat_flat Tensor(\"ner/block_1/Reshape:0\", shape=(?, 300), dtype=float32)\n",
      "input_to_pred Tensor(\"ner/block_1/hidden_dropout/dropout/mul:0\", shape=(?, 300), dtype=float32)\n",
      "proj_width 300\n",
      "scores Tensor(\"ner/block_1/output/scores:0\", shape=(?, 9), dtype=float32)\n",
      "unflat_scores Tensor(\"ner/block_1/output/Reshape:0\", shape=(?, 124, 9), dtype=float32)\n",
      "crf=True, creating SLL\n",
      "crf=True, creating SLL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up word embedding visualization\n",
      "Writing metadata\n",
      "Training epoch 1.\n",
      "Train (Loss 0.0473) (52.238 sec)\n",
      "Validation (F1 = 0.7337) (Acc 49389/51409 = 0.9607) (4.598 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 2.\n",
      "\t(last improvement @ 1)\n",
      "Train (Loss 0.0167) (47.281 sec)\n",
      "Validation (F1 = 0.8669) (Acc 50310/51409 = 0.9786) (3.938 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 3.\n",
      "\t(last improvement @ 2)\n",
      "Train (Loss 0.0101) (47.693 sec)\n",
      "Validation (F1 = 0.8793) (Acc 50474/51409 = 0.9818) (3.855 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 4.\n",
      "\t(last improvement @ 3)\n",
      "Train (Loss 0.0074) (47.472 sec)\n",
      "Validation (F1 = 0.9013) (Acc 50618/51409 = 0.9846) (3.882 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 5.\n",
      "\t(last improvement @ 4)\n",
      "Train (Loss 0.0056) (46.878 sec)\n",
      "Validation (F1 = 0.9030) (Acc 50609/51409 = 0.9844) (3.976 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 6.\n",
      "\t(last improvement @ 5)\n",
      "Train (Loss 0.0047) (47.598 sec)\n",
      "Validation (F1 = 0.9113) (Acc 50682/51409 = 0.9859) (3.879 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 7.\n",
      "\t(last improvement @ 6)\n",
      "Train (Loss 0.0037) (47.785 sec)\n",
      "Validation (F1 = 0.9181) (Acc 50740/51409 = 0.9870) (3.976 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 8.\n",
      "\t(last improvement @ 7)\n",
      "Train (Loss 0.0032) (47.500 sec)\n",
      "Validation (F1 = 0.9123) (Acc 50691/51409 = 0.9860) (4.150 sec)\n",
      "Training epoch 9.\n",
      "\t(last improvement @ 7)\n",
      "Train (Loss 0.0030) (47.631 sec)\n",
      "Validation (F1 = 0.9161) (Acc 50727/51409 = 0.9867) (3.987 sec)\n",
      "Training epoch 10.\n",
      "\t(last improvement @ 7)\n",
      "Train (Loss 0.0026) (47.222 sec)\n",
      "Validation (F1 = 0.9191) (Acc 50737/51409 = 0.9869) (3.941 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 11.\n",
      "\t(last improvement @ 10)\n",
      "Train (Loss 0.0024) (47.460 sec)\n",
      "Validation (F1 = 0.9198) (Acc 50751/51409 = 0.9872) (4.093 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 12.\n",
      "\t(last improvement @ 11)\n",
      "Train (Loss 0.0022) (47.481 sec)\n",
      "Validation (F1 = 0.9211) (Acc 50764/51409 = 0.9875) (4.000 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 13.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0021) (47.373 sec)\n",
      "Validation (F1 = 0.9176) (Acc 50752/51409 = 0.9872) (3.848 sec)\n",
      "Training epoch 14.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0019) (47.575 sec)\n",
      "Validation (F1 = 0.9158) (Acc 50714/51409 = 0.9865) (3.829 sec)\n",
      "Training epoch 15.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0018) (47.605 sec)\n",
      "Validation (F1 = 0.9205) (Acc 50747/51409 = 0.9871) (4.119 sec)\n",
      "Training epoch 16.\n",
      "\t(last improvement @ 12)\n",
      "Train (Loss 0.0016) (47.710 sec)\n",
      "Validation (F1 = 0.9220) (Acc 50759/51409 = 0.9874) (3.945 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 17.\n",
      "\t(last improvement @ 16)\n",
      "Train (Loss 0.0016) (47.597 sec)\n",
      "Validation (F1 = 0.9214) (Acc 50745/51409 = 0.9871) (4.140 sec)\n",
      "Training epoch 18.\n",
      "\t(last improvement @ 16)\n",
      "Train (Loss 0.0015) (48.440 sec)\n",
      "Validation (F1 = 0.9241) (Acc 50779/51409 = 0.9877) (4.063 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 19.\n",
      "\t(last improvement @ 18)\n",
      "Train (Loss 0.0014) (47.680 sec)\n",
      "Validation (F1 = 0.9250) (Acc 50779/51409 = 0.9877) (4.091 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 20.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0014) (47.375 sec)\n",
      "Validation (F1 = 0.9216) (Acc 50753/51409 = 0.9872) (4.000 sec)\n",
      "Training epoch 21.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0013) (48.523 sec)\n",
      "Validation (F1 = 0.9246) (Acc 50785/51409 = 0.9879) (3.887 sec)\n",
      "Training epoch 22.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0012) (48.358 sec)\n",
      "Validation (F1 = 0.9233) (Acc 50759/51409 = 0.9874) (3.733 sec)\n",
      "Training epoch 23.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0012) (48.413 sec)\n",
      "Validation (F1 = 0.9199) (Acc 50764/51409 = 0.9875) (3.723 sec)\n",
      "Training epoch 24.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0011) (48.532 sec)\n",
      "Validation (F1 = 0.9247) (Acc 50777/51409 = 0.9877) (3.835 sec)\n",
      "Training epoch 25.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0011) (48.205 sec)\n",
      "Validation (F1 = 0.9229) (Acc 50784/51409 = 0.9878) (3.987 sec)\n",
      "Training epoch 26.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0010) (48.260 sec)\n",
      "Validation (F1 = 0.9166) (Acc 50709/51409 = 0.9864) (3.918 sec)\n",
      "Training epoch 27.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0009) (47.907 sec)\n",
      "Validation (F1 = 0.9235) (Acc 50771/51409 = 0.9876) (4.007 sec)\n",
      "Training epoch 28.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0009) (48.148 sec)\n",
      "Validation (F1 = 0.9207) (Acc 50752/51409 = 0.9872) (3.867 sec)\n",
      "Training epoch 29.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0008) (48.159 sec)\n",
      "Validation (F1 = 0.9245) (Acc 50778/51409 = 0.9877) (3.808 sec)\n",
      "Training epoch 30.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0008) (48.401 sec)\n",
      "Validation (F1 = 0.9223) (Acc 50756/51409 = 0.9873) (3.981 sec)\n",
      "Training epoch 31.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0008) (48.256 sec)\n",
      "Validation (F1 = 0.9227) (Acc 50767/51409 = 0.9875) (3.901 sec)\n",
      "Training epoch 32.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0008) (48.657 sec)\n",
      "Validation (F1 = 0.9208) (Acc 50754/51409 = 0.9873) (3.747 sec)\n",
      "Training epoch 33.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0008) (48.374 sec)\n",
      "Validation (F1 = 0.9044) (Acc 50620/51409 = 0.9847) (3.910 sec)\n",
      "Training epoch 34.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0008) (48.172 sec)\n",
      "Validation (F1 = 0.9214) (Acc 50744/51409 = 0.9871) (3.926 sec)\n",
      "Training epoch 35.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0007) (48.150 sec)\n",
      "Validation (F1 = 0.9189) (Acc 50736/51409 = 0.9869) (3.911 sec)\n",
      "Training epoch 36.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0006) (48.056 sec)\n",
      "Validation (F1 = 0.9198) (Acc 50731/51409 = 0.9868) (3.798 sec)\n",
      "Training epoch 37.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0007) (48.260 sec)\n",
      "Validation (F1 = 0.9214) (Acc 50753/51409 = 0.9872) (3.936 sec)\n",
      "Training epoch 38.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0006) (48.463 sec)\n",
      "Validation (F1 = 0.9173) (Acc 50705/51409 = 0.9863) (4.050 sec)\n",
      "Training epoch 39.\n",
      "\t(last improvement @ 19)\n",
      "Train (Loss 0.0007) (48.428 sec)\n",
      "Validation (F1 = 0.9260) (Acc 50772/51409 = 0.9876) (3.978 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Test (F1 = 0.8815) (Acc 45607/46665 = 0.9773) (260.167 sec)\n",
      "Training epoch 40.\n",
      "\t(last improvement @ 39)\n",
      "Train (Loss 0.0007) (48.309 sec)\n",
      "Validation (F1 = 0.9274) (Acc 50787/51409 = 0.9879) (3.903 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 41.\n",
      "\t(last improvement @ 40)\n",
      "Train (Loss 0.0006) (48.380 sec)\n",
      "Validation (F1 = 0.9262) (Acc 50763/51409 = 0.9874) (4.025 sec)\n",
      "Training epoch 42.\n",
      "\t(last improvement @ 40)\n",
      "Train (Loss 0.0006) (48.132 sec)\n",
      "Validation (F1 = 0.9187) (Acc 50725/51409 = 0.9867) (3.990 sec)\n",
      "Training epoch 43.\n",
      "\t(last improvement @ 40)\n",
      "Train (Loss 0.0006) (48.371 sec)\n",
      "Validation (F1 = 0.9199) (Acc 50725/51409 = 0.9867) (3.763 sec)\n",
      "Training epoch 44.\n",
      "\t(last improvement @ 40)\n",
      "Train (Loss 0.0006) (48.227 sec)\n",
      "Validation (F1 = 0.9228) (Acc 50746/51409 = 0.9871) (3.761 sec)\n",
      "Training epoch 45.\n",
      "\t(last improvement @ 40)\n",
      "Train (Loss 0.0006) (48.094 sec)\n",
      "Validation (F1 = 0.9216) (Acc 50725/51409 = 0.9867) (3.901 sec)\n",
      "Training epoch 46.\n",
      "\t(last improvement @ 40)\n",
      "Train (Loss 0.0005) (48.120 sec)\n",
      "Validation (F1 = 0.9241) (Acc 50764/51409 = 0.9875) (4.134 sec)\n",
      "Training epoch 47.\n",
      "\t(last improvement @ 40)\n",
      "Train (Loss 0.0005) (48.210 sec)\n",
      "Validation (F1 = 0.9266) (Acc 50780/51409 = 0.9878) (3.897 sec)\n",
      "Training epoch 48.\n",
      "\t(last improvement @ 40)\n",
      "Train (Loss 0.0005) (48.049 sec)\n",
      "Validation (F1 = 0.9240) (Acc 50748/51409 = 0.9871) (3.853 sec)\n",
      "Training epoch 49.\n",
      "\t(last improvement @ 40)\n",
      "Train (Loss 0.0005) (48.294 sec)\n",
      "Validation (F1 = 0.9250) (Acc 50777/51409 = 0.9877) (3.898 sec)\n",
      "Training epoch 50.\n",
      "\t(last improvement @ 40)\n",
      "Train (Loss 0.0005) (48.256 sec)\n",
      "Validation (F1 = 0.9283) (Acc 50796/51409 = 0.9881) (3.940 sec)\n",
      "Highest dev F1 achieved yet -- writing model\n",
      "Training epoch 51.\n",
      "\t(last improvement @ 50)\n",
      "Train (Loss 0.0005) (48.447 sec)\n",
      "Validation (F1 = 0.9255) (Acc 50764/51409 = 0.9875) (3.820 sec)\n",
      "Training epoch 52.\n",
      "\t(last improvement @ 50)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.models.dconv3 import DConv\n",
    "from src.trainers.dconv3 import DConvTrainer\n",
    "\n",
    "trainer = DConvTrainer(None, DConv, 'conll-ner-dconv3-5')\n",
    "trainer.train('ner', ts, f2i, vs, es,\n",
    "             char_vec,\n",
    "             word_vec,\n",
    "             'eval',\n",
    "              batchsz=2**7,\n",
    "              optim='adam',\n",
    "              eta=0.0005,\n",
    "              epochs=500,\n",
    "              dropout=0.65, #not used in dconv3\n",
    "              patience=50,\n",
    "              cfiltsz='1,3,5,7',\n",
    "             maxlen=maxs,\n",
    "             maxw=maxw,\n",
    "              num_filt=300,\n",
    "             num_layers=3,\n",
    "             num_iterations=2,\n",
    "             fscore=1,\n",
    "              viz=1,\n",
    "             crf=True)\n",
    "\n",
    "#1 -- 86.55 dropout at .45 and 300 filters. stopped at epoc 24.\n",
    "#2 -- 86.7 dropout at .35, 350 filters.\n",
    "#3 -- added dilation 1 conv at end of block. 86.28\n",
    "#4 -- 3 layers, 3 iters. got much worse. peeked at 91.8 in training, started dropping.\n",
    "#5 -- layers in block should be 1,2,1 from strubell's code. lowered dropout to .15 -- layers would have been 1,1,1.\n",
    "#6 -- clipped gradients'\n",
    "#7 -- 3 layers, 2 iterations.\n",
    "#8 -- 2**6 batch size, to reflect strubell's config. hit 91.9 in validation, 84.6 in test.\n",
    "#9 -- upped dropout to .35 from .15. 86.49\n",
    "#10 -- drop to .65.\n",
    "#11 -- removed a layer from the block. from 3 (block's dilation should be 1,2,1) to 2, (dilation to 1,1).\n",
    "#   -- 86.57\n",
    "#12 -- increasing iterations. blocks have 3 layers again.\n",
    "#13 -- lowered blocks to 2 layers. decreased eta to .00005. 86.82\n",
    "#14 -- lowered adam's epsilon from 1e-8 to 1e-6. eta to .0005 from .00005, batch size to 2**7 from 2**6. 86.46\n",
    "\n",
    "#strubell's dconv3\n",
    "#1 -- 83.6 F1. wtf is going on.\n",
    "#2 -- batch size up to 2**7. seems misconfigured with two dropouts..\n",
    "#3 -- she has middle dropout and hidden dropout in the same place. middle drop is not used, though. i missed this. 87.09\n",
    "#4  -- added word dropout straight to the word vectors. dropout == .85.  87.54\n",
    "#   -- I forgot to account for dropout keep in evaluation....\n",
    "#5 -- setting dropout keep to 1 in evaluation using pkeep and word_keep."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
